{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('Model-v_FdXyLj': venv)",
   "metadata": {
    "interpreter": {
     "hash": "ce6c5e13534921f1b311ddcd67b5e37b3e390f1903127c3f1e22c4cea362d096"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
    "from skimage.transform import resize\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D,InputLayer, Input, concatenate ,RepeatVector ,Reshape ,UpSampling2D\n",
    "\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data from https://www.floydhub.com/emilwallner/datasets/colornet\n",
    "# download data an put it in a folder in the root called \"data\"\n",
    "items = []\n",
    "num = 0\n",
    "for file in os.listdir(\"./data/images/Train/\"):\n",
    "    if (num < 250):\n",
    "\n",
    "        img_array = img_to_array(load_img(\"./data/images/Train/\" + file))\n",
    "        items.append(img_array)\n",
    "    num += 1\n",
    "items = np.array(items)\n",
    "X_train = 1.0/255 * items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2, decode_predictions, preprocess_input\n",
    "inception = InceptionResNetV2(weights=None, include_top=True)\n",
    "inception.load_weights('/kaggle/input/dataset/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5')\n",
    "inception.graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To generate embeddings of 1000*1 by passing input images through InceptionResNetV2\n",
    "def create_inception_embedding(grayscaled_rgb):\n",
    "    grayscaled_rgb_resized = []\n",
    "    for i in grayscaled_rgb:\n",
    "        i = resize(i, (299, 299, 3), mode='constant')\n",
    "        grayscaled_rgb_resized.append(i)\n",
    "    grayscaled_rgb_resized = np.array(grayscaled_rgb_resized)\n",
    "    grayscaled_rgb_resized = preprocess_input(grayscaled_rgb_resized)\n",
    "    with inception.graph.as_default():\n",
    "        embed = inception.predict(grayscaled_rgb_resized)\n",
    "    return embed\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X_train (20000, 224, 224, 1)\n",
      "X_test (5000, 224, 224, 1)\n",
      "y_train (20000, 224, 224, 2)\n",
      "y_test (5000, 224, 224, 2)\n",
      "0\n",
      "255\n",
      "20\n",
      "226\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        rotation_range=20,\n",
    "        horizontal_flip=True\n",
    ")\n",
    "\n",
    "\n",
    "def image_train_datagen(batch_size):\n",
    "    for batch_imgs in train_datagen.flow(X_train, batch_size=batch_size):\n",
    "\n",
    "        grayscale_for_embeding = gray2rgb(rgb2gray(batch_imgs))\n",
    "    \n",
    "        embeding = embed_inception_prediction(grayscale_for_embeding)\n",
    "\n",
    "        lab_train = rgb2lab(batch_imgs)\n",
    "        l_batch = lab_train[:, :, :, 0]\n",
    "        l_batch = l_batch.reshape(l_batch.shape + (1,))\n",
    "\n",
    "        ab_batch = lab_train[:, :, :, 1:] / 128\n",
    "       \n",
    "        yield([l_batch, embeding], ab_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.layers import Conv2D, UpSampling2D, InputLayer, Conv2DTranspose, Input, Reshape, concatenate, Activation, Dense, Dropout, Flatten, RepeatVector\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedings_input = Input(shape=(1000, ))\n",
    "\n",
    "# using kears functional API to pass the outputs to each layer\n",
    "encoder_input = Input(shape=(256, 256, 1,))\n",
    "encoder1 = Conv2D(64, (3, 3), strides=(2, 2), activation='relu', padding='same')(encoder_input)\n",
    "encoder2 = Conv2D(128, (3, 3), activation='relu', padding='same')(encoder1)\n",
    "encoder3 = Conv2D(128, (3, 3), activation='relu', padding='same', strides=(2, 2))(encoder2)\n",
    "encoder4 = Conv2D(256, (3, 3), activation='relu', padding='same')(encoder3)\n",
    "encoder5 = Conv2D(256, (3, 3), activation='relu', padding='same', strides=(2, 2))(encoder4)\n",
    "encoder6 = Conv2D(512, (3, 3), activation='relu', padding='same')(encoder5)\n",
    "encoder7 = Conv2D(512, (3, 3), activation='relu', padding='same')(encoder6)\n",
    "encoder_output = Conv2D(512, (3, 3), activation='relu', padding='same')(encoder7)\n",
    "\n",
    "fusion1 = RepeatVector(32 * 32)(embedings_input)\n",
    "fusion2 = Reshape(([32, 32, 1000]))(fusion1)\n",
    "fusion3 = concatenate([encoder_output, fusion2], axis=3)\n",
    "fusion_output = Conv2D(256, (1, 1), activation='relu', padding='same')(fusion3)\n",
    "\n",
    "decoder1 = Conv2D(128, (3, 3), activation='relu', padding='same')(fusion_output)\n",
    "decoder2 = UpSampling2D((2, 2))(decoder1)\n",
    "decoder3 = Conv2D(64, (3, 3), activation='relu', padding='same')(decoder2)\n",
    "decoder4 = Conv2D(64, (3, 3), activation='relu', padding='same')(decoder3)\n",
    "decoder5 = UpSampling2D((2, 2))(decoder4)\n",
    "decoder6 = Conv2D(32, (3, 3), activation='relu', padding='same')(decoder5)\n",
    "decoder7 = Conv2D(16, (3, 3), activation='relu', padding='same')(decoder6)\n",
    "decoder8 = Conv2D(2, (3, 3), activation='tanh', padding='same')(decoder7)\n",
    "final_decoder_output = UpSampling2D((2, 2))(decoder8)\n",
    "\n",
    "myModel = Model(inputs=[encoder_input, embedings_input], outputs=final_decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './chroma_model.h5'\n",
    "\n",
    "make_checkpoint = ModelCheckpoint(model_path,\n",
    "                            monitor = \"val_loss\",\n",
    "                            mode=\"min\",\n",
    "                            save_best_only = True,\n",
    "                            verbose = 1)\n",
    "early_stoping = EarlyStopping(monitor='val_loss', mode='min', verbose = 1)\n",
    "\n",
    "myModel.compile(optimizer='adam', loss=\"mse\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_number = 10 # batch\n",
    "num_epochs = 1\n",
    "# total number of imgs / batch size\n",
    "steps_per_epoch_num = 929\n",
    "history = myModel.fit_generator(image_train_datagen(batch_size_number), epochs = num_epochs, steps_per_epoch=steps_per_epoch_num, verbose = 2, callbacks = [make_checkpoint,early_stoping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}